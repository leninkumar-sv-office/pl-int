================================================================================
STOCK PORTFOLIO DUPLICATE ANALYSIS
START HERE - Quick Navigation Guide
================================================================================

PURPOSE:
  This analysis identifies and reports 697 duplicate transaction rows across
  45 stock files in /sessions/vibrant-nice-lamport/mnt/pl/dumps/Stocks/

STATUS: Analysis complete. NO FILES MODIFIED. Read-only analysis only.

================================================================================
QUICK START (Pick One)
================================================================================

1. IF YOU WANT A 1-MINUTE OVERVIEW:
   → Read: README_ANALYSIS.txt (this directory)
   → Time: ~2 minutes
   → Covers: Key findings, most critical files, next steps

2. IF YOU WANT THE FULL PICTURE:
   → Read: DUPLICATE_ANALYSIS_REPORT.txt (this directory)
   → Time: ~10 minutes
   → Covers: Methodology, detailed findings, patterns, recommendations

3. IF YOU WANT QUICK REFERENCE:
   → Read: ANALYSIS_QUICK_REFERENCE.md (this directory)
   → Time: ~5 minutes
   → Covers: Key metrics, top files, pattern examples, cleanup strategy

4. IF YOU WANT TO ANALYZE THE DATA YOURSELF:
   → Open: duplicate_details.csv (in Excel or CSV viewer)
   → 697 rows of detailed duplicate information
   → Sortable by file, rows, dates, actions, prices, costs

5. IF YOU WANT TO RE-RUN THE ANALYSIS:
   → Run: python3 analyze_duplicates.py (detailed version)
   → Run: python3 analyze_duplicates_summary.py (summary version)

================================================================================
KEY FINDINGS AT A GLANCE
================================================================================

Files Analyzed:           67
Files with Duplicates:    45 (67%)
Total Duplicate Pairs:    697
Average per File:         15.5 pairs

TOP 5 PROBLEM FILES:
  1. Rallis India Ltd.xlsx                    101 duplicate pairs
  2. Rail Vikas Nigam.xlsx                     86 duplicate pairs
  3. High Energy Batteries (India) Ltd.xlsx    60 duplicate pairs
  4. Archive_Indian Railway Ctrng...           53 duplicate pairs
  5. Bombay Burmah Trading Corp Ltd.xlsx       52 duplicate pairs

DUPLICATE PATTERNS:
  - 50% have identical prices
  - 32% have different COST values
  - 16% have empty COST in newer row
  - 6% have different exchange (NSE vs BSE)

ROOT CAUSE:
  Pre-migration manual entries + Post-migration import duplicates from .xls
  load files

================================================================================
FILES IN THIS DIRECTORY
================================================================================

ANALYSIS REPORTS:
  ✓ README_ANALYSIS.txt
    → Start here for overview
    → Explains all analysis files
    → Quick start guide

  ✓ DUPLICATE_ANALYSIS_REPORT.txt
    → Full detailed analysis
    → Methodology and findings
    → Sample patterns and recommendations

  ✓ ANALYSIS_QUICK_REFERENCE.md
    → Quick metrics and reference
    → Pattern examples
    → Cleanup strategy

ANALYSIS DATA:
  ✓ duplicate_details.csv
    → All 697 duplicate pairs in tabular format
    → Sortable and filterable in Excel
    → Columns: Stock File, Row numbers, Date, Action, Price, Cost, Exchange

ANALYSIS SCRIPTS (runnable):
  ✓ analyze_duplicates.py
    → Generates detailed report with all duplicates
    → Usage: python3 analyze_duplicates.py

  ✓ analyze_duplicates_summary.py
    → Generates summary report with top 10 files
    → Usage: python3 analyze_duplicates_summary.py

THIS FILE:
  ✓ START_HERE.txt
    → Quick navigation guide (you are here)

================================================================================
DUPLICATE DETECTION LOGIC
================================================================================

For each pair of rows to be flagged as duplicates, they must have:

  1. Same DATE (exact match)
  2. Same ACTION: BUY or SELL (case-insensitive)
  3. Same QUANTITY (exact match)
  4. Similar PRICE (within 5% tolerance)

This "fuzzy key" approach captures the same transaction recorded twice but
with minor variations in price or cost calculation.

================================================================================
UNDERSTANDING THE DATA
================================================================================

DUPLICATE PAIRS CHARACTERISTICS:

Type 1: Exact Duplicates
  Row 5:  2026-02-04 | Buy 1 @ 911.70 | Cost: 918.16 | NSE
  Row 6:  2026-02-04 | Buy 1 @ 911.70 | Cost: 918.16 | NSE
  → Complete duplicate, same values
  → Action: Remove newer row (post-migration)

Type 2: Missing Cost
  Row 26: 2025-10-20 | Buy 40 @ 330.70 | Cost: 13321.59 | NSE
  Row 48: 2025-10-20 | Buy 40 @ 333.04 | Cost: None     | NSE
  → Newer row missing cost field
  → Action: Fill cost from older row OR remove newer row

Type 3: Different Exchange
  Row 6:  2025-10-17 | Buy 50 @ 136.50 | Cost: 6873.28 | BSE
  Row 7:  2025-10-17 | Buy 50 @ 137.47 | Cost: None     | NSE
  → Same transaction on different exchange
  → Action: Verify actual exchange, may be hedge or error

Type 4: Slight Variation
  Row 8:  2025-12-08 | Buy 8 @ 426.50 | Cost: 3436.31 | NSE
  Row 9:  2025-12-08 | Buy 8 @ 426.40 | Cost: 3435.51 | NSE
  → Price/cost differ <1%, likely rounding
  → Action: Keep older, higher-cost row (likely more accurate)

================================================================================
NEXT STEPS
================================================================================

STEP 1: Review the Analysis
  ✓ Start with README_ANALYSIS.txt (quick overview)
  ✓ Then read DUPLICATE_ANALYSIS_REPORT.txt (detailed findings)
  ✓ Check ANALYSIS_QUICK_REFERENCE.md (patterns and cleanup strategy)

STEP 2: Examine the Data
  ✓ Open duplicate_details.csv in Excel
  ✓ Sort by "Stock File" to group by company
  ✓ Filter by "Duplicate Pairs" to see problem files
  ✓ Review row numbers to understand pattern

STEP 3: Plan Your Cleanup
  ✓ Prioritize files with 50+ duplicates
  ✓ Decide keep/remove criteria for each type
  ✓ Plan how to handle missing COST values
  ✓ Verify exchange discrepancies

STEP 4: Make a Backup
  ✓ Before making any changes, backup your files
  ✓ Copy dump files to _backup directory
  ✓ Keep original as safety net

STEP 5: Clean the Data
  ✓ Use your favorite method (Excel, Python, etc.)
  ✓ Follow your planned cleanup criteria
  ✓ Test with small sample first

STEP 6: Validate Results
  ✓ Re-run analysis scripts to verify duplicates are removed
  ✓ Check affected files for data integrity
  ✓ Verify cost calculations are correct

================================================================================
IMPORTANT NOTES
================================================================================

✓ This analysis is READ-ONLY
  → No files have been modified
  → Safe to review and share

✓ Analysis methodology is sound
  → Fuzzy key matching (date+action+qty+price tolerance)
  → Only analyzed Buy/Sell transactions
  → Ignored other transaction types

✓ Files are self-contained
  → Can re-run analysis scripts anytime
  → No dependencies on external data
  → Results are reproducible

✓ Before cleanup
  → Make a backup of all dump files
  → Review findings carefully
  → Understand duplicate patterns
  → Create clear cleanup criteria

================================================================================
GETTING HELP
================================================================================

Q: How do I open the CSV file?
A: In Excel: File → Open → select duplicate_details.csv
   In Google Sheets: File → Open → Upload → select duplicate_details.csv

Q: How do I re-run the analysis?
A: $ python3 analyze_duplicates.py (detailed)
   $ python3 analyze_duplicates_summary.py (summary)

Q: Can I modify the analysis scripts?
A: Yes, the scripts are in this directory and fully editable.
   Both scripts read from the dump files and generate reports.

Q: What if I want to run analysis on a different set of files?
A: Edit the scripts and change the stocks_dir path at the bottom.

Q: How do I know which rows to keep?
A: Read DUPLICATE_ANALYSIS_REPORT.txt for recommendations.
   Generally: Keep pre-migration (earlier rows), remove post-migration.

Q: What if duplicates differ in cost but not price?
A: Investigate the source load files to determine correct cost.
   May indicate calculation errors during migration.

================================================================================
CONTACT & SUPPORT
================================================================================

All analysis files and scripts are located in:
  /sessions/vibrant-nice-lamport/mnt/pl/

Original dump files are in:
  /sessions/vibrant-nice-lamport/mnt/pl/dumps/Stocks/

The analysis is self-contained and can be re-run anytime without risk.

================================================================================
Analysis Generated: 2026-02-07
Status: COMPLETE - Ready for Review
================================================================================
